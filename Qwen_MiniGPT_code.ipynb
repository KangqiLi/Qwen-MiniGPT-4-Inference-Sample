{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fabfcf-c2c0-43b6-806b-07fc1a804b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e42818-80ae-4916-82c1-54d98249ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import glob\n",
    "from torchvision import transforms\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f155aaa-315b-4f5f-9138-64bf2a712ff3",
   "metadata": {},
   "source": [
    "### Data Loading Class for GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df774be7-96e2-4b52-8fa7-d3a1400c9f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSM8KDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        file_path = os.path.join(os.path.dirname(__file__), f\"{split}.jsonl\")\n",
    "        print(f\"Loading GSM8K dataset from {file_path}...\")\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.data = [json.loads(line) for line in f]\n",
    "        print(f\"Loaded {len(self.data)} examples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            \"question\": item[\"question\"],\n",
    "            \"answer\": item[\"answer\"]\n",
    "        }\n",
    "\n",
    "def get_gsm8k_dataloader(batch_size=32, split=\"train\"):\n",
    "    dataset = GSM8KDataset(split)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=(split == \"train\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9969e258-1225-42a6-8ec6-1166f0ceead6",
   "metadata": {},
   "source": [
    "### Data Loading Class for MMEDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb30566-b3c2-4031-a336-ff63ca81adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMEDataset(Dataset):\n",
    "    def __init__(self, root_dir, split=\"test\"):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.data = self.load_data()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),  \n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def load_data(self):\n",
    "        data = []\n",
    "        file_pattern = os.path.join(self.root_dir, f\"{self.split}-*-of-00004-*.parquet\")\n",
    "        parquet_files = sorted(glob.glob(file_pattern))\n",
    "        \n",
    "        if not parquet_files:\n",
    "            raise FileNotFoundError(f\"No parquet files found matching pattern: {file_pattern}\")\n",
    "\n",
    "        for file_path in parquet_files:\n",
    "            print(f\"Loading data from {file_path}\")\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "            df = pq.read_table(file_path).to_pandas()\n",
    "            data.append(df)\n",
    "        \n",
    "        data = pd.concat(data, ignore_index=True)\n",
    "        print(f\"Loaded {len(data)} examples.\")\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        \n",
    "        image_bytes = item['image']['bytes']\n",
    "        image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"question\": item[\"question\"],\n",
    "            \"answer\": item[\"answer\"],\n",
    "            \"question_id\": item[\"question_id\"],\n",
    "            \"category\": item[\"category\"]\n",
    "        }\n",
    "\n",
    "def custom_collate(batch):\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    questions = [item['question'] for item in batch]\n",
    "    answers = [item['answer'] for item in batch]\n",
    "    question_ids = [item['question_id'] for item in batch]\n",
    "    categories = [item['category'] for item in batch]\n",
    "\n",
    "    return {\n",
    "        'image': images,\n",
    "        'question': questions,\n",
    "        'answer': answers,\n",
    "        'question_id': question_ids,\n",
    "        'category': categories\n",
    "    }\n",
    "\n",
    "def get_mme_dataloader(root_dir, batch_size=1, split=\"test\"):\n",
    "    dataset = MMEDataset(root_dir, split)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd3848-0c8a-4dc9-839f-da6a8796691f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9fb69d5-56c0-47fb-8c50-39488a5069e1",
   "metadata": {},
   "source": [
    "### Qwen2-1.5B Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa6bb46-a31f-48a4-80d6-f66f3ebc8b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from data_loaders import get_gsm8k_dataloader\n",
    "import logging\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "def load_qwen_model():\n",
    "#    model_path = os.path.join(os.path.dirname(__file__), \"Qwen1.5-1.8B\")\n",
    "    model_path = \"/root/autodl-tmp/Qwen1.5-1.8B\"\n",
    "    print(f\"Attempting to load model from local path: {model_path}\")\n",
    "    \n",
    "    try:\n",
    "#        print(\"Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "#        print(\"Tokenizer loaded successfully.\")\n",
    "        \n",
    "#        print(\"Loading model... This may take a while.\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(device)\n",
    "#        print(f\"Model loaded successfully and moved to {device}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or tokenizer: {e}\")\n",
    "        raise\n",
    "    \n",
    "    return model, tokenizer, device\n",
    "\n",
    "def qwen_inference():\n",
    "    model, tokenizer, device = load_qwen_model()\n",
    "    dataloader = get_gsm8k_dataloader(batch_size=1, split=\"test\")\n",
    "    total_samples = len(dataloader)\n",
    "\n",
    "    results = []\n",
    "    process_single_sample = False\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):        \n",
    "        question = batch[\"question\"][0]\n",
    "        print(f\"  Question: {question[:50]}...\")\n",
    "        \n",
    "        prompt = f\"Question: {question}\\nAnswer:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=50, max_time=30,\n",
    "                                         num_return_sequences=1, do_sample=True, \n",
    "                                         temperature=0.7, top_p=0.95,\n",
    "                                         pad_token_id=tokenizer.eos_token_id)\n",
    "            \n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            answer = generated_text.split(\"Answer:\")[-1].strip()\n",
    "            print(f\"  Answer generated: {answer[:50]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error generating answer: {str(e)}\")\n",
    "            answer = \"Error: Failed to generate answer\"\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"generated_answer\": answer,\n",
    "            \"ground_truth\": batch[\"answer\"][0]\n",
    "        })\n",
    "        \n",
    "        torch.cuda.empty_cache()  \n",
    "        \n",
    "        if process_single_sample:\n",
    "            break \n",
    "    \n",
    "    # Monitor GPU memory usage\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = qwen_inference()\n",
    "    print(f\"Processed {len(results)} questions from GSM8K dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73db7e6-8fd1-4985-9044-89113c27d555",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dfb58d3-a7d6-4579-8eda-975760329f55",
   "metadata": {},
   "source": [
    "### MiniGPT-4 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee964a09-59a5-402d-a159-de2f24900a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from data_loaders import MMEDataset, get_mme_dataloader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "sys.path.append(os.path.expanduser(\"~/MiniGPT-4\"))\n",
    "\n",
    "from minigpt4.common.config import Config\n",
    "from minigpt4.common.registry import registry\n",
    "from minigpt4.conversation.conversation import Chat, CONV_VISION_Vicuna0, CONV_VISION_LLama2, StoppingCriteriaSub\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "def run_inference(chat, dataloader):\n",
    "    results = []\n",
    "    for batch in tqdm(dataloader, desc=\"Running inference\"):\n",
    "        image = batch[\"image\"][0]\n",
    "        question = batch[\"question\"][0]\n",
    "        ground_truth = batch[\"answer\"][0]\n",
    "\n",
    "        chat_state = CONV_VISION.copy()\n",
    "        chat_state.system = \"You are a helpful AI assistant.\"\n",
    "        chat.upload_img(image, chat_state, use_cache=False)\n",
    "        answer, _ = chat.answer(question, chat_state)\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"prediction\": answer\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_results(results, output_path):\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"question\", \"ground_truth\", \"prediction\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "def main():\n",
    "    cfg = Config(parse_args=False)\n",
    "    cfg.model_config = \"/MiniGPT-4/eval_configs/minigpt4_eval.yaml\"\n",
    "    cfg.options = [\"model.device='cuda'\"]\n",
    "    cfg.ckpt = \"prerained_minigpt4_7b.pth\"  \n",
    "\n",
    "    model_config = cfg.model_config\n",
    "    model_cls = registry.get_model_class(model_config.arch)\n",
    "    model = model_cls.from_config(model_config).to('cuda')\n",
    "\n",
    "    model.load_checkpoint(cfg.ckpt)\n",
    "\n",
    "    chat = Chat(model)\n",
    "\n",
    "    root_path = \".\"\n",
    "    dataloader = get_mme_dataloader(root_path, batch_size=1)\n",
    "\n",
    "    results = run_inference(chat, dataloader)\n",
    "\n",
    "    output_path = \"minigpt4_results.csv\"\n",
    "    save_results(results, output_path)\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d40ce6-9b07-43ba-9812-55a9d0aa33ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c4b09-1245-4ea2-99df-fcf8f79f385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from qwen_inference import qwen_inference\n",
    "#\n",
    "import time, argparse\n",
    "import sys\n",
    "import os, json, io\n",
    "sys.path.append(os.path.abspath('/root/MiniGPT-4'))\n",
    "import subprocess\n",
    "from data_loaders import MMEDataset\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "def create_result_table(results, model_name):\n",
    "    df = pd.DataFrame(results)\n",
    "    df[\"model\"] = model_name\n",
    "    return df\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Run inference on MME dataset using MiniGPT-4\")\n",
    "    parser.add_argument(\"--cfg_path\", default=\"model/MiniGPT-4/eval_configs/minigpt4_eval.yaml\", type=str,\n",
    "                        help=\"Path to MiniGPT-4 config file\")\n",
    "    parser.add_argument(\"--gpu_id\", type=int, default=0, help=\"GPU ID to use\")\n",
    "    parser.add_argument(\"--data_path\", default=\"/root\", type=str, help=\"Path to MME dataset\")\n",
    "    parser.add_argument(\"--output_path\", default=\"/root/minigpt4_results.csv\", type=str, \n",
    "                        help=\"Path to save results (CSV)\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    print(\"\\nRunning Qwen2-1.5b inference on GSM8K dataset...\")\n",
    "    qwen_results = qwen_inference()\n",
    "    qwen_df = create_result_table(qwen_results, \"Qwen2-1.5b\")\n",
    "    qwen_df.to_csv(\"qwen_results.csv\", index=False)\n",
    "    print(\"Qwen2-1.5b results saved to qwen_results.csv\")\n",
    "\n",
    "    print(\"\\nRunning MiniGPT-4 inference on MME dataset...\")\n",
    "    # minigpt4_start = time.time()\n",
    "    # args = parse_args()\n",
    "\n",
    "    # # Load MME dataset\n",
    "    # dataset = MMEDataset(args.data_path)\n",
    "\n",
    "    # # Initialize MiniGPT-4 inference\n",
    "    # minigpt4_model = MiniGPT4Inference(args.cfg_path, args.gpu_id)\n",
    "    # results = minigpt4_model.run_inference(dataset)\n",
    "\n",
    "    # # Convert results to DataFrame\n",
    "    # results_df = pd.DataFrame(results)\n",
    "\n",
    "    # # Save results to CSV\n",
    "    # results_df.to_csv(args.output_path, index=False)\n",
    "\n",
    "    # print(f\"Inference completed. Results saved to {args.output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
